{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Confusion Amongst Python Programmers\n",
    "\n",
    "An interactive introduction to data science workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Jupyter notebook.  It allows you to program interactively using both Python and Markdown (you can set individual cells of text to be either).  It has two modes: command mode and edit mode.  Hit `ESC` + `h` to see a Help menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Data Science Workflow\n",
    "\n",
    "1. Define the question and goals.\n",
    "2. Acquire the data.\n",
    "3. Scrub the data.\n",
    "4. Explore the data.\n",
    "5. Model the Data (if predictions needed).\n",
    "6. Communicate insights.\n",
    "7. Repeat and Refine as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the question and goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harriet Human-Resources, the VP in charge of hiring and training, comes to you one day and says\n",
    "\n",
    "> We need to make our internal training programs for teaching new hires programming languages better.  We’re going to put a team on it, but they need more information about what to build when teaching them new languages. We want to focus on Python first. I know you’re busy with 100 other things, but can you give us some preliminary insight at the end of the day?\n",
    "\n",
    "....\n",
    "\n",
    "Whew, what an ask!  Given the short timeline (deliver today) and haziness of what's asked, we will distill the question down to:\n",
    "\n",
    "**What aspects of Python programming present the most difficulties to programmers?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify and Acquire the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack Overflow is the world's premier software Q&A site.\n",
    "\n",
    "It organizes questions by topic tags.  One of these tags is \"Python\". There are almost one million questions on the Python tag: https://stackoverflow.com/questions/tagged/python.\n",
    "\n",
    "For the purposes of today's exercise, we are going to pretend that Stack Overflow's API is insufficient for our needs.  Instead, we are going to scrape the data in HTML format.\n",
    "\n",
    "You can do this using the Requests (as in HTTP) library. Requests provides a very simplistic API for making HTTP requests.\n",
    "\n",
    "In order to rapidly prototype, let's acquire just the first 5 pages of Python questions.  An examination of the Stack Overflow web app shows that this information lives at the URLs that look like this:\n",
    "\n",
    "https://stackoverflow.com/questions/tagged/python?page=5&sort=votes&pagesize=15\n",
    "\n",
    "See the `page=5` and `pagesize=15` query parameters?  \n",
    "\n",
    "Put all of this information together and use Python's built-in `open` function to scrape Stack Overflow's data to files in the `data/raw` diretory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page_range = range(1, 6)\n",
    "so_url = \"https://stackoverflow.com/questions/tagged/python?page={0}&sort=votes&pagesize=15\"\n",
    "raw_file = 'data/raw/PAGE_{0}.html'\n",
    "\n",
    "for page_number in page_range:\n",
    "        response = requests.get(so_url.format(page_number))\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(raw_file.format(page_number), 'w') as f:\n",
    "                f.write(response.text)  \n",
    "        else:\n",
    "            print(\"FAILED at {0} with status code {1}\".format(page_number, response.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clean/Wrangle the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is the most popular Python library for parsing HTML.  It parses HTML files into a tree-like Python data structure.\n",
    "\n",
    "It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('data/raw/PAGE_1.html', 'r') as f:\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    questions = soup.find_all(\"h1\")\n",
    "    print(questions[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the stack overflow page and think about the information contained on that page: What are the granular data points we want to extract?  What is the top-level real-world object? \n",
    "\n",
    "\n",
    "Here, it is the question object.  Some of the attributes of a question object that you can see on the page are:\n",
    "- question text\n",
    "- vote score\n",
    "- views \n",
    "- details\n",
    "- author\n",
    "- question details \n",
    "- date\n",
    "\n",
    "Let's think about this question from the top-down.  We basically want each of these question attributes grouped together in an object.  For purposes of developing our initial data structure, we can parse out 250 question objects into an array.  We basically want to be able to write code that looks like this:\n",
    "\n",
    "```\n",
    "dataset = []\n",
    "\n",
    "for i in page_range:\n",
    "    filename = \"data/raw/FILENAME_{}.html\".format(i)\n",
    "    qs = extract_question_objects(filename)\n",
    "    dataset.extend(qs)\n",
    "    \n",
    "```\n",
    "\n",
    "Let's write some supporting functions for this loop first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_info_from_summary(summary_div):\n",
    "    \"\"\"\n",
    "        :summary_div: A BS4 Tag Object \n",
    "    \"\"\"\n",
    "    qid = summary_div['id'].split(\"-\")[2]\n",
    "    text = summary_div.find('a', class_=\"question-hyperlink\").text\n",
    "    tags = [tag.text for tag in summary_div.find_all('a', class_=\"post-tag\")]\n",
    "    views = int(summary_div.find('div', class_=\"views\")['title'].split(\" \")[0].replace(\",\", \"\"))\n",
    "    votes = int(summary_div.find('span', class_=\"vote-count-post\").find('strong').text)\n",
    "\n",
    "    # data isn't always there\n",
    "    date = summary_div.find('span', class_='relativetime')\n",
    "    date_asked = date['title'] if date else None\n",
    "    \n",
    "    return [qid, views, text, tags, date_asked, votes]\n",
    "\n",
    "\n",
    "def extract_question_objects(relative_html_path):\n",
    "    \"\"\"\n",
    "        :relative_html_path: file to read and parse for stack overflow questions\n",
    "    \"\"\"\n",
    "    questions_objects = []\n",
    "    with open(relative_html_path, 'r') as f:\n",
    "    \n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        question_divs = soup.find_all(\"div\", class_=\"question-summary\")\n",
    "\n",
    "        for question in question_divs:\n",
    "                q_info = get_question_info_from_summary(question)\n",
    "                questions_objects.append(q_info)\n",
    "    \n",
    "    \n",
    "    return questions_objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run that top-down code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in page_range:\n",
    "    filename = \"data/raw/PAGE_{}.html\".format(i)\n",
    "    qs = extract_question_objects(filename)\n",
    "    dataset.extend(qs)\n",
    "\n",
    "#  uncomment for fun\n",
    "# print(len(dataset))\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things we want to do is get this data into a data structure known as a Pandas DataFrame.\n",
    "\n",
    "Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with relational (“labeled”) data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way toward this goal.\n",
    "\n",
    "The `DataFrame` is  the primary Pandas data structure.  They are great for exploring about tabular data - think columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['views', 'text', 'tags', 'date_asked', 'votes'] )\n",
    "\n",
    "for data in dataset:\n",
    "    qid, views, text, tags, date_asked, votes = data\n",
    "    df.loc[qid] = [views, text, tuple(np.array(tags)), date_asked, votes]\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas DataFrame API is rich and worth exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment line-by-line to explore:\n",
    "# df['tags']\n",
    "# df.columns\n",
    "# df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analaysis: Pandas Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames have some built-in data profilings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....but the `pandas-profiling` package is more complete.  One of the first things many data scientists do is run a profile on their dataframe.  Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of interesting things to notice here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real meat of the Stack Overflow data we have so far lay in question text.  Analyzing text is known as _natural language processing_.  One of the first things many data scientists do to examine text data is make a word cloud, measuring the importance of words by their relative frequencies.\n",
    "\n",
    "To get started with word clouds, we will use the Python `collections` library, most specifically its `Counter` data structure.  If you feed `Counter` an array of words, it will return a count of words frequencies.  Let's find the 100 most common words first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud 1: The 100 Most Common Words With `collections.Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "SO_words = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "     words = row.text.split(\" \")\n",
    "     for word in words:\n",
    "        SO_words.append(word)\n",
    "\n",
    "# print(SO_words)\n",
    "\n",
    "so_counter = Counter([word for word in SO_words])\n",
    "most_common = so_counter.most_common(100)\n",
    "\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this with a wordcloud, measuring by frequency.  We will use two libraries, `matplotlib` and `wordcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordCloud expects a dictionary mapping words to frequencies (i.e. strings to floats).  Let's massage the data in that direction with a dict comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_word_frequencies = {word: frequency for word, frequency in most_common}\n",
    "\n",
    "# print(python_word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the word cloud is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=most_common)\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.savefig('wordcloud.png') # Does this work??? Just saves black image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like hot garbage.   \n",
    "\n",
    "Let's use a list of **stop words** as a filter when we're compiling our count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud #2: Removing Stop Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_counter = Counter([word for word in SO_words if word.lower() not in stop])\n",
    "\n",
    "most_common = so_counter.most_common(100)\n",
    "\n",
    "# print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same visualization code to see if this is any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=most_common)\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting better.  \n",
    "\n",
    "Now we can bring in **subject-matter expertise** (AKA **domain expertise**) and filter for the most relevant Python stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud #4: You are the Python and Stack Overflow subject-matter expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build on `nltk`'s list of stop words.  Filter out the non-helpful and trivially helpful stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SO_stopwords = [\n",
    "    'python', \n",
    "    'python?', \n",
    "    'using',\n",
    "    'how',\n",
    "    'what',\n",
    "    'why',\n",
    "    'how',\n",
    "    'way',\n",
    "    '[closed]',\n",
    "    '[duplicate]',\n",
    "]\n",
    "\n",
    "Python_stopwords = set(stopwords.words('english') + SO_stopwords)\n",
    "\n",
    "# print(Python_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, run the code again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_counter = Counter([word for word in SO_words if word.lower() not in Python_stopwords])\n",
    "\n",
    "most_common = so_counter.most_common(100)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=most_common)\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking of Pandas DataFrames, let's try getting more information into this word cloud by weighting these words with something less crude than sheer word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud #4: Split-Apply-Combine With Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votes and views numbers are available for this data.  Let's weight each word by an aggregate score of `views + votes` instead of just word frequency.  Since we're just exploring, let's slice and aggregate this data into a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(columns=['word', 'score'])\n",
    "\n",
    "# Use X as an auto-ID incrementer\n",
    "x = -1\n",
    " \n",
    "for i, row in df.iterrows():\n",
    "    words = row.text.split(\" \")\n",
    "    score = float(row.views) + float(row.votes)\n",
    "    for word in words:\n",
    "        if word.lower() not in Python_stopwords:\n",
    "            x = x + 1 \n",
    "            df_words.loc[x] = [word, score]\n",
    "\n",
    "# df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a technique known as split-apply-combine to aggreggate all the same words together and combine their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_word = df_words.groupby('word')\n",
    "scores = grouped_by_word.agg('sum')\n",
    "top_100 = scores.sort_values('score')[-100:]\n",
    "\n",
    "# top_100\n",
    "\n",
    "frequencies = [(i, word.score)  for i, word in top_100.iterrows() ]\n",
    "\n",
    "# top_100\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=frequencies)\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  What other improvements could we make?  Not just improving the code, but how could we extract and communicate more information from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: Tag Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "import itertools\n",
    "\n",
    "# slice off the Python in each one of these\n",
    "tags = [row.tags[1:] for i, row in df.iterrows()]\n",
    "\n",
    "x = 0\n",
    "\n",
    "for question_tags in tags:\n",
    "    \n",
    "    if x < 100:\n",
    "        for tag in question_tags:\n",
    "            G.add_node(tag)\n",
    "\n",
    "        for tag1, tag2 in itertools.combinations(question_tags, 2):\n",
    "            if 'list' in (tag1, tag2):\n",
    "                G.add_edge(tag1, tag2)\n",
    "        \n",
    "        x = x + 1\n",
    "\n",
    "nx.draw(G, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model the data if preditions are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not predicting anything today. 🌞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Communicate insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are pretty ready to walk into our end-of-day meeting.  As John Tukey said, exploratory data analysis is an attitude.  Our attitude has now shifted to \"I have a same-day meeting to deliver some insight.\"  At this point, you are prepared with:\n",
    "* A profile of Stack Overflow question fields, which gives business intelligence about what sort of data is readily available;\n",
    "* A word cloud that anybody can understand (visualization #1)\n",
    "* A network graph visualization of related topics Python programmers have quesitons about (visualization #2) \n",
    "* A Jupyter notebook full of replicable code to be iterated and improved upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
