{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Confusion Amongst Python Programmers\n",
    "\n",
    "An interactive introduction to data science workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Jupyter notebook.  It allows you to program interactively using both Python and Markdown (you can set individual cells of text to be either).  It has two modes: command mode and edit mode.  Hit `ESC` + `h` to see a Help menu.  `shift` + `enter` will run an individual cell.  You can run all cells immediately from both the Cell and Kernel sub-menus above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harriet Human-Resources, the VP in charge of hiring and training, comes to you one day and says:\n",
    "\n",
    "> We need to make our internal training programs for recent hires better.  We‚Äôre going to put a team on it, but they need more information how to teach them new languages. We want to focus on Python first.\n",
    "   \n",
    "   > I know you‚Äôre busy with 100 other things, but can you give us some preliminary insight at the end of the day?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Data Science Workflow\n",
    "\n",
    "1. Define the question and goals.\n",
    "2. Acquire the data.\n",
    "3. Scrub the data.\n",
    "4. Explore the data.\n",
    "5. Model the Data (if predictions needed).\n",
    "6. Communicate insights.\n",
    "7. Repeat and Refine as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the question and goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> **What aspects of Python programming present the most difficulties to programmers?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify and Acquire the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack Overflow is the world's premier software Q&A site.\n",
    "\n",
    "It organizes questions by topic tags.  One of these tags is \"Python\". There are almost one million questions on the Python tag: https://stackoverflow.com/questions/tagged/python.\n",
    "\n",
    "For the purposes of today's exercise, we are going to pretend that Stack Overflow's API is insufficient for our needs.  Instead, we are going to scrape the data in HTML format.\n",
    "\n",
    "You can do this using the Requests (as in HTTP) library. Requests provides a very simplistic API for making HTTP requests.\n",
    "\n",
    "In order to rapidly prototype, let's acquire just the first 5 pages of Python questions.  An examination of the Stack Overflow web app shows that this information lives at the URLs that look like this:\n",
    "\n",
    "https://stackoverflow.com/questions/tagged/python?page=5&sort=votes&pagesize=15\n",
    "\n",
    "See the `page=5` and `pagesize=15` query parameters?  \n",
    "\n",
    "Put all of this information together and use Python's built-in `open` function to scrape Stack Overflow's data to files in the `data/raw` diretory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page_range = range(1, 6)\n",
    "so_url = \"https://stackoverflow.com/questions/tagged/python?page={0}&sort=votes&pagesize=15\"\n",
    "raw_file = 'data/raw/PAGE_{0}.html'\n",
    "\n",
    "for page_number in page_range:\n",
    "    response = requests.get(so_url.format(page_number))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        f = open(raw_file.format(page_number), 'w')\n",
    "        f.write(response.text)  \n",
    "        f.close()\n",
    "    else:\n",
    "        print(\"FAILED at {0} with status code {1}\".format(page_number, response.status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clean/Wrangle the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is the most popular Python library for parsing HTML.  It parses HTML files into a tree-like Python data structure.\n",
    "\n",
    "It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('data/raw/PAGE_1.html', 'r') as f:\n",
    "    soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "    page_title = soup.find_all(\"h1\")\n",
    "    print(page_title[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the Stack Overflow Pythonpage and think about the information contained on that page: What are the granular data points we want to extract?  What is the top-level real-world object? \n",
    "\n",
    "\n",
    "Here, it is the question object.  Some of the attributes of a question object that you can see on the page are:\n",
    "- question text\n",
    "- vote score\n",
    "- views \n",
    "- details\n",
    "- author\n",
    "- question details \n",
    "- date\n",
    "\n",
    "Let's think about this question from the top-down.  We basically want each of these question attributes grouped together in an object.  For purposes of developing our initial data structure, we can parse out 250 question objects into an array.  We basically want to be able to write code that looks like this:\n",
    "\n",
    "```\n",
    "dataset = []\n",
    "\n",
    "for i in page_range:\n",
    "    filename = \"data/raw/FILENAME_{}.html\".format(i)\n",
    "    qs = extract_question_objects(filename)\n",
    "    dataset.extend(qs)\n",
    "    \n",
    "```\n",
    "\n",
    "Let's write some supporting functions for this loop first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_info_from_summary(summary_div):\n",
    "    qid = summary_div['id'].split(\"-\")[2]\n",
    "    text = summary_div.find('a', class_=\"question-hyperlink\").text\n",
    "    tags = [tag.text for tag in summary_div.find_all('a', class_=\"post-tag\")]\n",
    "    views = int(summary_div.find('div', class_=\"views\")['title'].split(\" \")[0].replace(\",\", \"\"))\n",
    "    votes = int(summary_div.find('span', class_=\"vote-count-post\").find('strong').text)\n",
    "\n",
    "    # data isn't always there\n",
    "    date = summary_div.find('span', class_='relativetime')\n",
    "    date_asked = date['title'] if date else None\n",
    "    \n",
    "    return [qid, views, text, tags, date_asked, votes]\n",
    "\n",
    "\n",
    "def extract_question_objects(relative_html_path):\n",
    "    questions_objects = []\n",
    "    \n",
    "    with open(relative_html_path, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        \n",
    "    question_divs = soup.find_all(\"div\", class_=\"question-summary\")\n",
    "    \n",
    "    for question in question_divs:\n",
    "            q_info = get_question_info_from_summary(question)\n",
    "            questions_objects.append(q_info)\n",
    "    \n",
    "    return questions_objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run that top-down code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in page_range:\n",
    "    filename = \"data/raw/PAGE_{}.html\".format(i)\n",
    "    qs = extract_question_objects(filename)\n",
    "    dataset.extend(qs)\n",
    "\n",
    "#  uncomment for fun\n",
    "# print(len(dataset))\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things we want to do is get this data into a data structure known as a Pandas DataFrame.\n",
    "\n",
    "Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with relational (‚Äúlabeled‚Äù) data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis /manipulation tool available in any language. It is well on its way toward this goal.\n",
    "\n",
    "The `DataFrame` is  the primary Pandas data structure.  They are great for exploring about tabular data - think columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['views', 'text', 'tags', 'date_asked', 'votes'] )\n",
    "\n",
    "for data in dataset:\n",
    "    qid, views, text, tags, date_asked, votes = data\n",
    "    df.loc[qid] = [views, text, tuple(np.array(tags)), date_asked, votes]\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas DataFrame API is rich and worth exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment line-by-line to explore:\n",
    "# df['tags']\n",
    "# df.columns\n",
    "# df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analaysis: Pandas Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames have some built-in data profilings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....but the `pandas-profiling` package is more complete.  One of the first things many data scientists do is run a profile on their dataframe.  Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of interesting things to notice here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real meat of the Stack Overflow data we have so far lay in question text.  Analyzing text is one aspect of _natural language processing_.  One of the first things many data scientists do to examine text data is make a word cloud, measuring the importance of words by their relative frequencies.\n",
    "\n",
    "To get started with word clouds, we will use the Python `collections` library, most specifically its `Counter` data structure.  If you feed `Counter` an array of words, it will return a count of words frequencies.  Let's find the 100 most common words first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud 1: The 100 Most Common Words With `collections.Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "SO_words = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "     words = row.text.split(\" \")\n",
    "     for word in words:\n",
    "        SO_words.append(word)\n",
    "\n",
    "so_counter = Counter([word for word in SO_words])\n",
    "most_common = so_counter.most_common(100)\n",
    "\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this with a wordcloud, measuring by frequency.  We will use two libraries, `matplotlib` and `wordcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Newer versions of WordCloud (which you're likely to have if you manually installed the\n",
    "## packages rather than using the spec file) expect a dictionary mapping words to frequencies \n",
    "## (i.e. strings to floats).  You can uncomment the following line of code if this is the case\n",
    "## for you.  \n",
    "\n",
    "# most_common = {word: frequency for word, frequency in most_common}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the word cloud is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=most_common)\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('wordcloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like hot garbage.   \n",
    "\n",
    "Let's use a list of **stop words** as a filter when we're compiling our count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud #2: Removing Stop Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_words = [word for word in SO_words if word.lower() not in stop]\n",
    "so_counter = Counter(so_words)\n",
    "most_common = so_counter.most_common(100)\n",
    "\n",
    "# print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same visualization code to see if this is any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=most_common)\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('wordcloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting better.  \n",
    "\n",
    "Now we can bring in **subject-matter expertise** (AKA **domain expertise**) and filter for the most relevant Python stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud #3: You are the Python and Stack Overflow subject-matter expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build on `nltk`'s list of stop words.  Filter out the non-helpful and trivially helpful stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SO_stopwords = [\n",
    "    'python', \n",
    "    'python?', \n",
    "    'using',\n",
    "    'how',\n",
    "    'what',\n",
    "    'why',\n",
    "    'how',\n",
    "    'way',\n",
    "    '[closed]',\n",
    "    '[duplicate]',\n",
    "]\n",
    "\n",
    "Python_stopwords = set(stopwords.words('english') + SO_stopwords)\n",
    "\n",
    "# print(Python_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, run the code again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_counter = Counter([word for word in SO_words if word.lower() not in Python_stopwords])\n",
    "\n",
    "most_common = so_counter.most_common(100)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=most_common)\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking better! I see Pandas and DataFrames in there... speaking of: let's try getting more information into this word cloud by weighting these words with something less crude than sheer word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Cloud #4: Split-Apply-Combine With Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votes and views numbers are available for this data.  Let's weight each word by an aggregate score of `views + votes` instead of just word frequency.  Since we're just exploring, let's slice and aggregate this data into a new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(columns=['word', 'score'])\n",
    "\n",
    "# Use X as an auto-ID incrementer\n",
    "x = -1\n",
    " \n",
    "for i, row in df.iterrows():\n",
    "    words = row.text.split(\" \")\n",
    "    score = float(row.views) + float(row.votes)\n",
    "    for word in words:\n",
    "        if word.lower() not in Python_stopwords:\n",
    "            x = x + 1 \n",
    "            df_words.loc[x] = [word, score]\n",
    "\n",
    "df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use a technique known as split-apply-combine to aggreggate all the same words together and combine their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_word = df_words.groupby('word')\n",
    "scores = grouped_by_word.agg('sum')\n",
    "\n",
    "top_100 = scores.sort_values('score')[-100:]\n",
    "frequencies = [(i, word.score)  for i, word in top_100.iterrows()]\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400)\n",
    "wordcloud.generate_from_frequencies(frequencies=frequencies)\n",
    "plt.figure(figsize=(20,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('wordcloud.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  What other improvements could we make?  Not just improving the code, but how could we extract and communicate more information from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS Exploratory Data Analysis: Tag Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the 50 most popular question tags that aren't Python and see how they relate to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "# Get the 25 most popular tags to use as a filter\n",
    "\n",
    "all_tags = []\n",
    "\n",
    "row_tags = [row.tags for i, row in df.iterrows()]\n",
    "\n",
    "for tags in row_tags:\n",
    "    for tag in tags:\n",
    "        if tag != 'python':\n",
    "            all_tags.append(tag)\n",
    "\n",
    "top_25_tuples = Counter(all_tags).most_common(25)\n",
    "\n",
    "top_25 = []\n",
    "for tag, count in top_25_tuples:\n",
    "    top_25.append(tag)\n",
    "\n",
    "top_25\n",
    "\n",
    "if 'list' in top_25:\n",
    "    print('whoo')\n",
    "\n",
    "\n",
    "# # # # # # # #\n",
    "    \n",
    "import itertools\n",
    "\n",
    "# # slice off the Python in each one of these\n",
    "tags = [row.tags[1:] for i, row in df.iterrows()]\n",
    "\n",
    "x = 0\n",
    "\n",
    "for question_tags in tags:\n",
    "    for tag in question_tags:\n",
    "        if tag in top_25:\n",
    "            G.add_node(tag)\n",
    "\n",
    "    for tag1, tag2 in itertools.combinations(question_tags, 2):\n",
    "        if tag1 not in top_25 or tag2 not in top_25:\n",
    "            break\n",
    "        else: \n",
    "            G.add_edge(tag1, tag2)\n",
    "\n",
    "    x = x + 1\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "nx.draw(G, with_labels=True, font_size=40, node_size=500, scale=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not looking so good... but our meeting is coming up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model the data if preditions are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not predicting anything today. üåû"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Communicate insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are pretty ready to walk into our end-of-day meeting.  As John Tukey said, exploratory data analysis is an attitude.  Our attitude has now shifted to \"I have a same-day meeting to deliver some insight.\"  At this point, you are prepared with:\n",
    "\n",
    "* A profile of Stack Overflow question fields, which gives business intelligence about what sort of data is readily available;\n",
    "* A word cloud that anybody can understand (visualization #1)\n",
    "* A network graph visualization of related topics Python programmers have quesitons about (visualization #2) \n",
    "* A Jupyter notebook full of replicable code to be iterated and improved upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Challenges\n",
    "\n",
    "0. Create reusable functions from the code in this notebook.  This would make the notebook a much stronger resource for your company.\n",
    "1. Play with the final visualization (the tag network graph) to make it a better visualization.\n",
    "2. Organize the original data into multiple DataFrames -- start by making tags its own DataFrame.\n",
    "3. Find new ways to apply the split-apply-combine paradigm to this data.\n",
    "4. What information did we not scrape from the questions page?  Go scrape that data.\n",
    "5. Scrape the individual questions pages - the answers contain a tremendous amount of data as well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
